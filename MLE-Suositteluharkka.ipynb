{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Spark session\n",
    "Spark session is \"the gateway to the structured data processing\".\n",
    "It can be used to create datasets, dataframes, user defined functions and execute SQL.\n",
    "It replaces SQLContext used in previous versions of Apache Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker_pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# This enables s3 support in Spark. You may need to restart the kernel!\n",
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Teemuko mle\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", classpath) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the CSV-data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movie ratings data\n",
    "filePath = \"s3a://sagemaker-tmukoo/ratings-5000.csv\"\n",
    "#filePath = \"s3a://sagemaker-tmukoo/ratings.csv\"\n",
    "\n",
    "\n",
    "ratings = spark.read.load(filePath, format=\"csv\", inferSchema=\"true\", header=\"true\")\n",
    "#df=spark.read.csv(filePath,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movie metadata\n",
    "filePath = \"s3a://sagemaker-tmukoo/movies_metadata.csv\"\n",
    "\n",
    "\n",
    "movies = spark.read.load(filePath, format=\"csv\", inferSchema=\"true\", header=\"true\")\n",
    "#df=spark.read.csv(filePath,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId=1, movieId=110, rating=1.0, timestamp=1425941529),\n",
       " Row(userId=1, movieId=147, rating=4.5, timestamp=1425942435),\n",
       " Row(userId=1, movieId=858, rating=5.0, timestamp=1425941523)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- adult: string (nullable = true)\n",
      " |-- belongs_to_collection: string (nullable = true)\n",
      " |-- budget: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      " |-- homepage: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- imdb_id: string (nullable = true)\n",
      " |-- original_language: string (nullable = true)\n",
      " |-- original_title: string (nullable = true)\n",
      " |-- overview: string (nullable = true)\n",
      " |-- popularity: string (nullable = true)\n",
      " |-- poster_path: string (nullable = true)\n",
      " |-- production_companies: string (nullable = true)\n",
      " |-- production_countries: string (nullable = true)\n",
      " |-- release_date: string (nullable = true)\n",
      " |-- revenue: string (nullable = true)\n",
      " |-- runtime: string (nullable = true)\n",
      " |-- spoken_languages: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- tagline: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- video: string (nullable = true)\n",
      " |-- vote_average: string (nullable = true)\n",
      " |-- vote_count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ratings.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many distinct userIds?\n",
    "ratings.select('userId').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2347"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many distinct movieIds?\n",
    "ratings.select('movieId').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc(\"font\",size=15)\n",
    "ratings.select('rating').toPandas().rating.sort_values().value_counts(sort=False).plot(kind='bar')\n",
    "plt.title('Rating distribution')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall statistics \n",
    "from pyspark.sql.functions import mean, min, max\n",
    "ratings.select([mean('rating'), min('rating'), max('rating')]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+--------------+\n",
      "|movieId|               title|       avg(rating)|count(movieId)|\n",
      "+-------+--------------------+------------------+--------------+\n",
      "|   1408|    Cutthroat Island|               4.0|             2|\n",
      "|    524|              Casino|              2.25|             2|\n",
      "|      5|          Four Rooms|               3.5|             2|\n",
      "|    902| {'name': 'Victoi...|             3.875|             4|\n",
      "|     63|      Twelve Monkeys|               2.5|             1|\n",
      "|   2054|  Mr. Holland's Opus|2.5833333333333335|             6|\n",
      "|    880|      Antonia's Line|               2.5|             2|\n",
      "|    568|           Apollo 13|               3.0|             1|\n",
      "|   1873|      Beyond Rangoon|               2.5|             2|\n",
      "|   3512|Under Siege 2: Da...|               3.0|             1|\n",
      "|   1909|    Don Juan DeMarco|               4.0|             3|\n",
      "|   4954|           Drop Zone|               3.0|             1|\n",
      "|    628|Interview with th...|               4.0|             2|\n",
      "|     11|           Star Wars|               3.0|             1|\n",
      "|   3036|               123.0|               3.0|             1|\n",
      "|   1945|                Nell|               4.5|             2|\n",
      "|    527|  Once Were Warriors| 4.269230769230769|            13|\n",
      "|   6950|            Outbreak|               2.5|             1|\n",
      "|    101|[{'iso_639_1': 'e...|               5.0|             1|\n",
      "|    110|   Three Colors: Red|3.8333333333333335|             9|\n",
      "+-------+--------------------+------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average rating per movie and rating counts\n",
    "average_ratings=ratings.groupBy(ratings.movieId).agg({\"movieId\": \"count\", \"rating\": \"avg\"}).orderBy([\"count(movieId)\"],ascending=0)\n",
    "average_ratings.join(movies, movies.id == ratings.movieId).select('movieId','title','avg(rating)','count(movieId)').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(movieId=5, rating=3.0),\n",
       " Row(movieId=25, rating=3.0),\n",
       " Row(movieId=32, rating=2.0),\n",
       " Row(movieId=58, rating=3.0),\n",
       " Row(movieId=64, rating=4.0),\n",
       " Row(movieId=79, rating=4.0),\n",
       " Row(movieId=141, rating=3.0),\n",
       " Row(movieId=260, rating=4.0),\n",
       " Row(movieId=339, rating=5.0),\n",
       " Row(movieId=377, rating=4.0),\n",
       " Row(movieId=605, rating=4.0),\n",
       " Row(movieId=628, rating=4.0),\n",
       " Row(movieId=648, rating=4.0),\n",
       " Row(movieId=762, rating=3.0),\n",
       " Row(movieId=780, rating=3.0),\n",
       " Row(movieId=786, rating=1.0),\n",
       " Row(movieId=788, rating=1.0),\n",
       " Row(movieId=1210, rating=4.0),\n",
       " Row(movieId=1233, rating=4.0),\n",
       " Row(movieId=1356, rating=5.0),\n",
       " Row(movieId=1475, rating=3.0),\n",
       " Row(movieId=1552, rating=2.0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find movieId:s and ratings for a specific user\n",
    "def users_ratings_df(user):\n",
    "    return ratings.filter(ratings[\"userId\"]==user).select('movieId','rating')\n",
    "#.collect()\n",
    "\n",
    "users_ratings_df(2).collect()\n",
    "#for movieId,rating in users_ratings(3):\n",
    "#    print(movieId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivoting data for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a movieId vs userId average rating pivot (using average if the movie has been rated twice) \n",
    "ratings_pivot=ratings.groupBy('userId')\\\n",
    ".pivot('movieId')\\\n",
    ".agg({\"rating\": \"avg\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#specific_film_ratings=ratings_pivot.select('858')\n",
    "#from pyspark.sql import functions as F\n",
    "#ratings_pivot.filter(specific_film_ratings._1.isNotNull)\n",
    "ratings_pivot.filter(ratings_pivot['858'].isNotNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning to the rescue!\n",
    "Note! SparkML will eventually replace MLlib. => don't use MLlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS based Collaborative Filtering\n",
    "https://spark.apache.org/docs/2.2.0/ml-collaborative-filtering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 3.0873995150333977\n"
     ]
    }
   ],
   "source": [
    "# DON'T USE! from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "\n",
    "#lines = spark.read.text(\"data/mllib/als/sample_movielens_ratings.txt\").rdd\n",
    "#parts = lines.map(lambda row: row.value.split(\"::\"))\n",
    "#ratingsRDD = parts.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]), rating=float(p[2]), timestamp=long(p[3])))\n",
    "#ratings = spark.createDataFrame(ratingsRDD)\n",
    "\n",
    "(training, test) = ratings.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Build the recommendation model using ALS on the training data\n",
    "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)\n",
    "\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n",
    "\n",
    "# Generate top 10 movie recommendations for each user\n",
    "userRecs = model.recommendForAllUsers(10)\n",
    "# Generate top 10 user recommendations for each movie\n",
    "movieRecs = model.recommendForAllItems(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId=31, recommendations=[Row(movieId=4973, rating=8.152472496032715), Row(movieId=58559, rating=7.841063022613525), Row(movieId=68157, rating=7.421996116638184), Row(movieId=7153, rating=7.082418918609619), Row(movieId=4993, rating=6.970481872558594), Row(movieId=2791, rating=6.839625358581543), Row(movieId=1080, rating=6.831757545471191), Row(movieId=2571, rating=6.804441928863525), Row(movieId=4886, rating=6.6022443771362305), Row(movieId=1136, rating=6.383308410644531)]),\n",
       " Row(userId=34, recommendations=[Row(movieId=318, rating=7.932281970977783), Row(movieId=1073, rating=6.483544826507568), Row(movieId=47, rating=5.941385746002197), Row(movieId=2804, rating=5.867222309112549), Row(movieId=1210, rating=5.41481876373291), Row(movieId=592, rating=5.408281326293945), Row(movieId=5952, rating=5.395789623260498), Row(movieId=457, rating=5.299190044403076), Row(movieId=6539, rating=5.297870635986328), Row(movieId=2502, rating=5.294939041137695)]),\n",
       " Row(userId=28, recommendations=[Row(movieId=1089, rating=8.28123950958252), Row(movieId=6377, rating=7.2404961585998535), Row(movieId=1923, rating=6.529565811157227), Row(movieId=3897, rating=6.418732166290283), Row(movieId=356, rating=6.409822940826416), Row(movieId=2273, rating=6.402120113372803), Row(movieId=608, rating=6.3270649909973145), Row(movieId=1208, rating=6.143359184265137), Row(movieId=50, rating=6.127578258514404), Row(movieId=293, rating=6.123276710510254)])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tämä kai olisi kullekin käyttäjälle ALS:n suosittelemat leffat\n",
    "userRecs.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(movieId=1580, recommendations=[Row(userId=38, rating=6.308559894561768), Row(userId=1, rating=5.330371856689453), Row(userId=41, rating=5.036062240600586), Row(userId=15, rating=5.0004072189331055), Row(userId=4, rating=4.731659889221191), Row(userId=46, rating=4.526854991912842), Row(userId=16, rating=4.504119873046875), Row(userId=31, rating=4.449660301208496), Row(userId=33, rating=4.372954368591309), Row(userId=9, rating=4.342031478881836)]),\n",
       " Row(movieId=471, recommendations=[Row(userId=24, rating=2.992443084716797), Row(userId=44, rating=2.3984858989715576), Row(userId=13, rating=2.364163875579834), Row(userId=4, rating=1.8684618473052979), Row(userId=28, rating=1.5902862548828125), Row(userId=38, rating=1.580761432647705), Row(userId=15, rating=1.5459280014038086), Row(userId=17, rating=1.5233920812606812), Row(userId=25, rating=1.290235161781311), Row(userId=19, rating=1.238815426826477)]),\n",
       " Row(movieId=1591, recommendations=[Row(userId=8, rating=3.9521634578704834), Row(userId=7, rating=3.9238572120666504), Row(userId=49, rating=2.996939182281494), Row(userId=34, rating=2.9301276206970215), Row(userId=33, rating=2.1117897033691406), Row(userId=32, rating=2.0053250789642334), Row(userId=46, rating=1.9697754383087158), Row(userId=43, rating=1.9396640062332153), Row(userId=47, rating=1.57483971118927), Row(userId=2, rating=1.4258456230163574)])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieRecs.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|    title|\n",
      "+---+---------+\n",
      "|862|Toy Story|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.select('id','title').filter(\"title like '%Toy Story'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(recommendations=[Row(userId=38, rating=6.308559894561768), Row(userId=1, rating=5.330371856689453), Row(userId=41, rating=5.036062240600586), Row(userId=15, rating=5.0004072189331055), Row(userId=4, rating=4.731659889221191), Row(userId=46, rating=4.526854991912842), Row(userId=16, rating=4.504119873046875), Row(userId=31, rating=4.449660301208496), Row(userId=33, rating=4.372954368591309), Row(userId=9, rating=4.342031478881836)])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mitä tää data kertoo? Elokuvan perusteella lähinnä mun makua olevat käyttäjät?\n",
    "# Onko tää joku Tinderin korvike?\n",
    "movieRecs.filter(\"movieId = 1580\").select('recommendations').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Palataan takaisin manuaaliseen recommendation enginen koodaukseen..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pearson correlation test\n",
    "# Result... nulls are counted as zeros in Spark => crap results\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
    "        (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
    "        (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
    "        (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
    "\n",
    "data = [(Vectors.dense([7,6,7,4,5,4]),),\n",
    "        (Vectors.sparse(6, [(0,6), (1,7), (3,4), (4,3), (5,4) ]),),\n",
    "        (Vectors.sparse(6, [(1,3),(2,3),(3,1),(4,1)]),),\n",
    "        (Vectors.dense([1,2,2,3,3,4]),),\n",
    "        (Vectors.sparse(6, [(0,1),(2,1),(3,2),(4,3),(5,3)]),),\n",
    "        (Vectors.sparse(6, [(0,5),(1,4),(3,3),(5,4)]),)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "r1 = Correlation.corr(df, \"features\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n",
    "\n",
    "r2 = Correlation.corr(df, \"features\", \"spearman\").head()\n",
    "print(\"Spearman correlation matrix:\\n\" + str(r2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n"
     ]
    }
   ],
   "source": [
    "# Testing spark sum aggregation for user's ratings\n",
    "both_rated=[5,25,32]\n",
    "person1_preferences_sum = users_ratings_df(2).filter(users_ratings_df(2).movieId.isin(both_rated))\\\n",
    ".agg({\"rating\": \"sum\"}).select('sum(rating)').collect()[0][0]\n",
    "#.groupBy().power('rating',2)\n",
    "#    \n",
    "#    .agg({\"movieId\": \"sum\"})\n",
    "print(person1_preferences_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This installs pyarrow to the conda_python3 kernel environment\n",
    "# conda install --yes --name python3 --channel conda-forge pyarrow\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "# Use pandas_udf to define a Pandas UDF\n",
    "@pandas_udf('double', PandasUDFType.SCALAR)\n",
    "# Input/output are both a pandas.Series of doubles\n",
    "\n",
    "def pandas_plus_one(v):\n",
    "    return v + 1\n",
    "\n",
    "@pandas_udf('double', PandasUDFType.SCALAR)\n",
    "# Input/output are both a pandas.Series of doubles\n",
    "\n",
    "def pandas_power2(v):\n",
    "    return v*v\n",
    "\n",
    "#df.withColumn('v2', pandas_plus_one(df.v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'DataFrame' and 'DataFrame'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-5cb51ec4d4b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mperson1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mperson1_square_preferences_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m                                     \u001b[0musers_ratings_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperson1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musers_ratings_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperson1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmovieId\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboth_rated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m                                      \u001b[0musers_ratings_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperson1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musers_ratings_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperson1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmovieId\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboth_rated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m)\u001b[0m                                      \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#print(person1_square_preferences_sum)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'DataFrame' and 'DataFrame'"
     ]
    }
   ],
   "source": [
    "person1=2\n",
    "person1_square_preferences_sum = sum(\\\n",
    "                                     users_ratings_df(person1).\\\n",
    "filter(users_ratings_df(person1).movieId.isin(both_rated)).select('rating')*\\\n",
    "                                      users_ratings_df(person1).\\\n",
    "filter(users_ratings_df(person1).movieId.isin(both_rated)).select('rating')\\\n",
    "                                      )\n",
    "                                     \n",
    "#print(person1_square_preferences_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+---------+---------+---------------+\n",
      "|movieId|rating1|rating2|rating1^2|rating2^2|ratings_product|\n",
      "+-------+-------+-------+---------+---------+---------------+\n",
      "|     25|    3.0|    3.0|      9.0|      9.0|            9.0|\n",
      "|     32|    2.0|    2.0|      4.0|      4.0|            4.0|\n",
      "|     58|    3.0|    3.0|      9.0|      9.0|            9.0|\n",
      "|    260|    4.0|    4.0|     16.0|     16.0|           16.0|\n",
      "|    377|    4.0|    4.0|     16.0|     16.0|           16.0|\n",
      "|    648|    4.0|    4.0|     16.0|     16.0|           16.0|\n",
      "|    788|    1.0|    1.0|      1.0|      1.0|            1.0|\n",
      "|   1210|    4.0|    4.0|     16.0|     16.0|           16.0|\n",
      "|   1233|    4.0|    4.0|     16.0|     16.0|           16.0|\n",
      "+-------+-------+-------+---------+---------+---------------+\n",
      "\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from math import sqrt,pow \n",
    "\n",
    "def pearson_correlation(person1,person2):\n",
    "\n",
    "##### To get both rated items\n",
    "#    both_rated = {}\n",
    "    #for item in dataset[person1]:\n",
    "    #    if item in dataset[person2]:\n",
    "    #        both_rated[item] = 1\n",
    " \n",
    "#    for movieId,rating in users_ratings_df(person1).collect():\n",
    "#        if movieId in users_ratings_df(person2).collect():\n",
    "#            both_rated[movieId] = 1\n",
    "\n",
    "#    person1_ratings = users_ratings_df(34).alias('df1')\n",
    "#    person2_ratings = users_ratings_df(2).alias('df2')\n",
    "#    both_rated=person1_ratings.join(person2_ratings, df1.movieId == df2.movieId)\\\n",
    "#    .select('df1.*').select('movieId').rdd.flatMap(lambda x: x).collect()\n",
    "    \n",
    "#    return both_rated\n",
    "###########################\n",
    "\n",
    "\n",
    "#    number_of_ratings = len(both_rated)\n",
    "    \n",
    "    # Checking for number of ratings in common\n",
    "#    if number_of_ratings == 0:\n",
    "#        return 0\n",
    " \n",
    "    # Add up all the preferences of each user\n",
    "#    person1_preferences_sum = sum([dataset[person1][item] for item in both_rated])\n",
    "#    person2_preferences_sum = sum([dataset[person2][item] for item in both_rated])\n",
    "\n",
    "#    person1_preferences_sum = users_ratings_df(person1).\\\n",
    "#    filter(users_ratings_df(person1).movieId.isin(both_rated))\\\n",
    "#    .agg({\"rating\": \"sum\",\"rating\": \"avg\"}).select('sum(rating)').collect()[0][0]\n",
    "        \n",
    "#    person2_preferences_sum = users_ratings(person2).\\\n",
    "#    filter(users_ratings_df(person2).movieId.isin(both_rated))\\\n",
    "#    .agg({\"rating\": \"sum\"}).select('sum(rating)').collect()[0][0]\n",
    "    \n",
    "    \n",
    "    # Sum up the squares of preferences of each user\n",
    "#    person1_square_preferences_sum = sum([pow(dataset[person1][item],2) for item in both_rated])\n",
    "#    person2_square_preferences_sum = sum([pow(dataset[person2][item],2) for item in both_rated])\n",
    "\n",
    "#    person1_square_preferences_sum = sum(users_ratings_df(person1).\\\n",
    "#    filter(users_ratings_df(person1).movieId.isin(both_rated)).select('rating'),2)\n",
    " \n",
    " \n",
    "    # Sum up the product value of both preferences for each item\n",
    "#    product_sum_of_both_users = sum([dataset[person1][item] * dataset[person2][item] for item in both_rated])\n",
    "\n",
    "    \n",
    "    # Spark way of calculating all the above:\n",
    "    # Method to find common MovieIds\n",
    "    \n",
    "    person1_ratings = users_ratings_df(person1).alias('df1')\n",
    "    person2_ratings = users_ratings_df(person2).alias('df2')\n",
    "\n",
    "    # pow(df2.rating, 2) does not work in here, for some reason.\n",
    "    common_ratings = person1_ratings.join(person2_ratings, df1.movieId == df2.movieId).select(\\\n",
    "                                                                         df1.movieId, \\\n",
    "                                                                         df1.rating.alias('rating1'), \\\n",
    "                                                                         df2.rating.alias('rating2'), \\\n",
    "                                                                         (df1.rating*df1.rating).alias('rating1^2'),\\\n",
    "                                                                        (df2.rating*df2.rating).alias('rating2^2'),\\\n",
    "                                                                         (df1.rating*df2.rating).alias('ratings_product')\\\n",
    "                                                                        )\n",
    "    common_ratings.show()\n",
    "    number_of_ratings = common_ratings.count()\n",
    "    \n",
    "    # Checking for number of ratings in common\n",
    "    if number_of_ratings == 0:\n",
    "        return 0\n",
    "  \n",
    " \n",
    "    common_ratings_agg = common_ratings.agg({\\\n",
    "                                             \"movieId\":\"count\",\\\n",
    "                                             \"rating1\": \"sum\",\\\n",
    "                                             \"rating2\": \"sum\",\\\n",
    "                                             \"rating1^2\": \"sum\",\\\n",
    "                                             \"rating2^2\": \"sum\",\\\n",
    "                                             \"ratings_product\": \"sum\"}).collect()[0]\n",
    "    \n",
    "    # Unpacking the numebers from the named tuple:\n",
    "    (number_of_ratings2,\\\n",
    "     person1_preferences_sum,\\\n",
    "     person2_preferences_sum,\\\n",
    "     person1_square_preferences_sum,\\\n",
    "     person2_square_preferences_sum,\\\n",
    "     product_sum_of_both_users) = \\\n",
    "    (common_ratings_agg[\"count(movieId)\"],\\\n",
    "     common_ratings_agg[\"sum(rating1)\"],\\\n",
    "     common_ratings_agg[\"sum(rating2)\"],\\\n",
    "     common_ratings_agg[\"sum(rating1^2)\"],\\\n",
    "     common_ratings_agg[\"sum(rating2^2)\"],\\\n",
    "     common_ratings_agg[\"sum(ratings_product)\"],\\\n",
    "    )\n",
    "    \n",
    "    #return person2_preferences_sum\n",
    "    #return number_of_ratings2\n",
    "\n",
    "    # Calculate the pearson score\n",
    "    numerator_value = product_sum_of_both_users - (person1_preferences_sum*person2_preferences_sum/number_of_ratings)\n",
    "    denominator_value = sqrt((person1_square_preferences_sum - pow(person1_preferences_sum,2)/number_of_ratings) * (person2_square_preferences_sum -pow(person2_preferences_sum,2)/number_of_ratings))\n",
    "    if denominator_value == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        r = numerator_value/denominator_value\n",
    "        return r\n",
    "    \n",
    "print(pearson_correlation(2,24))\n",
    "#print pearson_correlation('Lisa Rose','Gene Seymour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_ratings=person1_ratings.join(person2_ratings, df1.movieId == df2.movieId).select(\\\n",
    "                                                                         df1.movieId, \\\n",
    "                                                                         df1.rating.alias('rating1'), \\\n",
    "                                                                         df2.rating.alias('rating2'), \\\n",
    "                                                                         (pow(df1.rating,2)).alias('rating1^2'),\\\n",
    "                                                                        (pow(df2.rating,2)).alias('rating2^2'),\\\n",
    "                                                                         (df1.rating*df2.rating).alias('ratings_product')\\\n",
    "                                                                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<b'contains(movieId, 5)'>\n",
      "Column<b'contains(movieId, 25)'>\n",
      "Column<b'contains(movieId, 32)'>\n",
      "Column<b'contains(movieId, 58)'>\n",
      "Column<b'contains(movieId, 64)'>\n",
      "Column<b'contains(movieId, 79)'>\n",
      "Column<b'contains(movieId, 141)'>\n",
      "Column<b'contains(movieId, 260)'>\n",
      "Column<b'contains(movieId, 339)'>\n",
      "Column<b'contains(movieId, 377)'>\n",
      "Column<b'contains(movieId, 605)'>\n",
      "Column<b'contains(movieId, 628)'>\n",
      "Column<b'contains(movieId, 648)'>\n",
      "Column<b'contains(movieId, 762)'>\n",
      "Column<b'contains(movieId, 780)'>\n",
      "Column<b'contains(movieId, 786)'>\n",
      "Column<b'contains(movieId, 788)'>\n",
      "Column<b'contains(movieId, 1210)'>\n",
      "Column<b'contains(movieId, 1233)'>\n",
      "Column<b'contains(movieId, 1356)'>\n",
      "Column<b'contains(movieId, 1475)'>\n",
      "Column<b'contains(movieId, 1552)'>\n"
     ]
    }
   ],
   "source": [
    "for mo,ra in users_ratings_df(2).collect():\n",
    "    print(users_ratings_df(34).movieId.contains(mo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rating2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-333-a1bc0f6a3f03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mperson2_ratings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0musers_ratings_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#.alias('df2')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcommon_ratings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperson1_ratings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperson2_ratings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'movieId'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rating2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"movieId\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperson1_ratings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmovieId\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrating2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#,(pow(rating2,2)).alias('rating1^2'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#common_ratings = person1_ratings.withColumnRenamed(\"rating\", \"rating1\").join(person2_ratings.withColumnRenamed(\"rating\", \"rating2\"), \\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rating2' is not defined"
     ]
    }
   ],
   "source": [
    "# Method to find common MovieIds\n",
    "from pyspark.sql.functions import *\n",
    "from math import sqrt \n",
    "person1_ratings = users_ratings_df(34) #.alias('df1')\n",
    "person2_ratings = users_ratings_df(2) #.alias('df2')\n",
    "\n",
    "common_ratings = person1_ratings.join(person2_ratings.toDF('movieId','rating2'), \"movieId\")\\\n",
    ".select(person1_ratings.movieId,\"rating2\")#,(pow(rating2,2)).alias('rating1^2'))\n",
    "#common_ratings = person1_ratings.withColumnRenamed(\"rating\", \"rating1\").join(person2_ratings.withColumnRenamed(\"rating\", \"rating2\"), \\\n",
    "\n",
    "                                                                             #                                                        person1_ratings.movieId == person2_ratings.movieId).\\\n",
    "#select(person1_ratings.movieId,\"rating1\",(\"rating2\"))#,(pow(rating1,2)).alias('rating1^2'))#,\\\n",
    "#                                                                        (pow(rating2,2)).alias('rating2^2'),\\\n",
    "#                                                                         (rating1*rating2).alias('ratings_product')\\\n",
    "#)\n",
    "#.select(\\\n",
    "#                                                                         df1.movieId, \\\n",
    "#                                                                        df1.rating1.alias('rating1'))#, \\\n",
    "  #                                                                       df2.rating2.alias('rating2'), \\\n",
    "  #                                                                       (pow(df1.rating,2)).alias('rating1^2'),\\\n",
    "  #                                                                      (pow(df2.rating,2)).alias('rating2^2'),\\\n",
    "  #                                                                       (df1.rating*df2.rating).alias('ratings_product')\\\n",
    "  #                                                                      )\n",
    "\n",
    "common_ratings.show()\n",
    "#(numb,x,y,z,f,g)=\n",
    "foo=common_ratings.agg({\"movieId\":\"count\",\\\n",
    "                                \"rating1\": \"sum\",\\\n",
    "                    \"rating2\": \"sum\",\\\n",
    "                    \"rating1^2\": \"sum\",\\\n",
    "                    \"rating2^2\": \"sum\",\\\n",
    "                    \"ratings_product\": \"sum\"}) #.collect()[0]\n",
    "#foo[\"sum(rating1)\"]\n",
    "foo.show()\n",
    "#.select('movieId').rdd.flatMap(lambda x: x).collect()\n",
    "#numb,sqrt(x),type(y),z,f,g,common_ratings.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From https://blog.epigno.systems/2018/02/21/machine-learning-with-pyspark-feature-selection/\n",
    "#prepare the data\n",
    "features = [\"temperature\", \"exhaust_vacuum\", \"ambient_pressure\", \"relative_humidity\"]\n",
    "lr_data = data.select(col(\"energy_output\").alias(\"label\"), *features).dropna()\n",
    "\n",
    "vector = VectorAssembler(inputCols=columns, outputCol=\"features\")\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "\n",
    "#stages = [vector, scaler]\n",
    "stages = [scaler]\n",
    "\n",
    "pipe = Pipeline(stages=stages)\n",
    "\n",
    "# we'll be using this data frame\n",
    "data_for_correlation = pipe.fit(lr_data).transform(lr_data).select(\"scaled_features\")\n",
    "\n",
    "\n",
    "#The correlation step\n",
    "correlation = Correlation.corr(data_for_correlation, \"scaled_features\", \"pearson\").collect()[0][0].toArray()\n",
    "\n",
    " # rename _1, _2 ... columns to their original name\n",
    "df = pd.DataFrame(correlation)\n",
    "df[\"features\"] = pd.Series(columns)\n",
    "\n",
    " # let's see the results\n",
    "display(spark.createDataFrame(df, schema=columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "spark.createDataFrame(data).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count mean ratings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pearsonCorr = Correlation.corr(df, \"features\").collect()[0][0]\n",
    "print(str(pearsonCorr).replace('nan', 'NaN'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#import numpy as np\n",
    "#np.average((Vectors.sparse(4, [(0, 2.0), (3, 2.0)]),))\n",
    "#neo=spark.createDataFrame([(Vectors.sparse(4, [(0, 2.0), (3, 2.0)]),)], [\"rating\"])\n",
    "\n",
    "from pyspark.sql.functions import mean, min, max\n",
    "#neo.select([mean('rating'), min('rating'), max('rating')]).show()\n",
    "\n",
    "df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
