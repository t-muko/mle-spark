{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Spark session\n",
    "Spark session is \"the gateway to the structured data processing\".\n",
    "It can be used to create datasets, dataframes, user defined functions and execute SQL.\n",
    "It replaces SQLContext used in previous versions of Apache Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker_pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# This enables s3 support in Spark. You may need to restart the kernel!\n",
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Teemuko mle\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", classpath) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the CSV-data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movie ratings data\n",
    "filePath = \"s3a://sagemaker-tmukoo/ratings-5000.csv\"\n",
    "#filePath = \"s3a://sagemaker-tmukoo/ratings.csv\"\n",
    "\n",
    "\n",
    "ratings = spark.read.load(filePath, format=\"csv\", inferSchema=\"true\", header=\"true\")\n",
    "#df=spark.read.csv(filePath,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movie metadata\n",
    "filePath = \"s3a://sagemaker-tmukoo/movies_metadata.csv\"\n",
    "\n",
    "\n",
    "movies = spark.read.load(filePath, format=\"csv\", inferSchema=\"true\", header=\"true\")\n",
    "#df=spark.read.csv(filePath,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId=1, movieId=110, rating=1.0, timestamp=1425941529),\n",
       " Row(userId=1, movieId=147, rating=4.5, timestamp=1425942435),\n",
       " Row(userId=1, movieId=858, rating=5.0, timestamp=1425941523)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- adult: string (nullable = true)\n",
      " |-- belongs_to_collection: string (nullable = true)\n",
      " |-- budget: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      " |-- homepage: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- imdb_id: string (nullable = true)\n",
      " |-- original_language: string (nullable = true)\n",
      " |-- original_title: string (nullable = true)\n",
      " |-- overview: string (nullable = true)\n",
      " |-- popularity: string (nullable = true)\n",
      " |-- poster_path: string (nullable = true)\n",
      " |-- production_companies: string (nullable = true)\n",
      " |-- production_countries: string (nullable = true)\n",
      " |-- release_date: string (nullable = true)\n",
      " |-- revenue: string (nullable = true)\n",
      " |-- runtime: string (nullable = true)\n",
      " |-- spoken_languages: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- tagline: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- video: string (nullable = true)\n",
      " |-- vote_average: string (nullable = true)\n",
      " |-- vote_count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ratings.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many distinct userIds?\n",
    "ratings.select('userId').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2347"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many distinct movieIds?\n",
    "ratings.select('movieId').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc(\"font\",size=15)\n",
    "ratings.select('rating').toPandas().rating.sort_values().value_counts(sort=False).plot(kind='bar')\n",
    "plt.title('Rating distribution')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall statistics \n",
    "from pyspark.sql.functions import mean, min, max\n",
    "ratings.select([mean('rating'), min('rating'), max('rating')]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+--------------+\n",
      "|movieId|               title|       avg(rating)|count(movieId)|\n",
      "+-------+--------------------+------------------+--------------+\n",
      "|   1408|    Cutthroat Island|               4.0|             2|\n",
      "|    524|              Casino|              2.25|             2|\n",
      "|      5|          Four Rooms|               3.5|             2|\n",
      "|    902| {'name': 'Victoi...|             3.875|             4|\n",
      "|     63|      Twelve Monkeys|               2.5|             1|\n",
      "|   2054|  Mr. Holland's Opus|2.5833333333333335|             6|\n",
      "|    880|      Antonia's Line|               2.5|             2|\n",
      "|    568|           Apollo 13|               3.0|             1|\n",
      "|   1873|      Beyond Rangoon|               2.5|             2|\n",
      "|   3512|Under Siege 2: Da...|               3.0|             1|\n",
      "|   1909|    Don Juan DeMarco|               4.0|             3|\n",
      "|   4954|           Drop Zone|               3.0|             1|\n",
      "|    628|Interview with th...|               4.0|             2|\n",
      "|     11|           Star Wars|               3.0|             1|\n",
      "|   3036|               123.0|               3.0|             1|\n",
      "|   1945|                Nell|               4.5|             2|\n",
      "|    527|  Once Were Warriors| 4.269230769230769|            13|\n",
      "|   6950|            Outbreak|               2.5|             1|\n",
      "|    101|[{'iso_639_1': 'e...|               5.0|             1|\n",
      "|    110|   Three Colors: Red|3.8333333333333335|             9|\n",
      "+-------+--------------------+------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average rating per movie and rating counts\n",
    "average_ratings=ratings.groupBy(ratings.movieId).agg({\"movieId\": \"count\", \"rating\": \"avg\"}).orderBy([\"count(movieId)\"],ascending=0)\n",
    "average_ratings.join(movies, movies.id == ratings.movieId).select('movieId','title','avg(rating)','count(movieId)').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(movieId=5, rating=3.0),\n",
       " Row(movieId=25, rating=3.0),\n",
       " Row(movieId=32, rating=2.0),\n",
       " Row(movieId=58, rating=3.0),\n",
       " Row(movieId=64, rating=4.0),\n",
       " Row(movieId=79, rating=4.0),\n",
       " Row(movieId=141, rating=3.0),\n",
       " Row(movieId=260, rating=4.0),\n",
       " Row(movieId=339, rating=5.0),\n",
       " Row(movieId=377, rating=4.0),\n",
       " Row(movieId=605, rating=4.0),\n",
       " Row(movieId=628, rating=4.0),\n",
       " Row(movieId=648, rating=4.0),\n",
       " Row(movieId=762, rating=3.0),\n",
       " Row(movieId=780, rating=3.0),\n",
       " Row(movieId=786, rating=1.0),\n",
       " Row(movieId=788, rating=1.0),\n",
       " Row(movieId=1210, rating=4.0),\n",
       " Row(movieId=1233, rating=4.0),\n",
       " Row(movieId=1356, rating=5.0),\n",
       " Row(movieId=1475, rating=3.0),\n",
       " Row(movieId=1552, rating=2.0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find movieId:s and ratings for a specific user\n",
    "def users_ratings_df(user):\n",
    "    return ratings.filter(ratings[\"userId\"]==user).select('movieId','rating')\n",
    "#.collect()\n",
    "\n",
    "users_ratings_df(2).collect()\n",
    "#for movieId,rating in users_ratings(3):\n",
    "#    print(movieId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivoting data for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a movieId vs userId average rating pivot (using average if the movie has been rated twice) \n",
    "ratings_pivot=ratings.groupBy('userId')\\\n",
    ".pivot('movieId')\\\n",
    ".agg({\"rating\": \"avg\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#specific_film_ratings=ratings_pivot.select('858')\n",
    "#from pyspark.sql import functions as F\n",
    "#ratings_pivot.filter(specific_film_ratings._1.isNotNull)\n",
    "ratings_pivot.filter(ratings_pivot['858'].isNotNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning to the rescue!\n",
    "Note! SparkML will eventually replace MLlib. => don't use MLlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS based Collaborative Filtering\n",
    "https://spark.apache.org/docs/2.2.0/ml-collaborative-filtering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 2.8625903877937433\n"
     ]
    }
   ],
   "source": [
    "# DON'T USE! from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "\n",
    "#lines = spark.read.text(\"data/mllib/als/sample_movielens_ratings.txt\").rdd\n",
    "#parts = lines.map(lambda row: row.value.split(\"::\"))\n",
    "#ratingsRDD = parts.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]), rating=float(p[2]), timestamp=long(p[3])))\n",
    "#ratings = spark.createDataFrame(ratingsRDD)\n",
    "\n",
    "(training, test) = ratings.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Build the recommendation model using ALS on the training data\n",
    "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)\n",
    "\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n",
    "\n",
    "# Generate top 10 movie recommendations for each user\n",
    "userRecs = model.recommendForAllUsers(10)\n",
    "# Generate top 10 user recommendations for each movie\n",
    "movieRecs = model.recommendForAllItems(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId=31, recommendations=[Row(movieId=2018, rating=4.999802589416504), Row(movieId=3755, rating=4.981315612792969), Row(movieId=1377, rating=4.500730037689209), Row(movieId=1358, rating=4.5001749992370605), Row(movieId=2006, rating=4.499944686889648), Row(movieId=2087, rating=4.499881267547607), Row(movieId=520, rating=3.9991908073425293), Row(movieId=2109, rating=3.1323153972625732), Row(movieId=168252, rating=3.102997303009033), Row(movieId=2294, rating=2.9996719360351562)]),\n",
       " Row(userId=34, recommendations=[Row(movieId=102125, rating=6.458173751831055), Row(movieId=2804, rating=5.75777530670166), Row(movieId=593, rating=5.495298385620117), Row(movieId=7153, rating=5.1707329750061035), Row(movieId=1196, rating=5.09976053237915), Row(movieId=60069, rating=5.090648651123047), Row(movieId=168252, rating=5.069170951843262), Row(movieId=1250, rating=5.0691328048706055), Row(movieId=44555, rating=5.056756496429443), Row(movieId=3471, rating=5.056451797485352)]),\n",
       " Row(userId=28, recommendations=[Row(movieId=2324, rating=12.168180465698242), Row(movieId=969, rating=11.411534309387207), Row(movieId=3000, rating=10.939973831176758), Row(movieId=2019, rating=10.890886306762695), Row(movieId=2686, rating=10.644259452819824), Row(movieId=4979, rating=10.443951606750488), Row(movieId=902, rating=10.282062530517578), Row(movieId=904, rating=10.238629341125488), Row(movieId=1231, rating=10.226531982421875), Row(movieId=6711, rating=10.193713188171387)])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tämä kai olisi kullekin käyttäjälle ALS:n suosittelemat leffat\n",
    "userRecs.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(movieId=1580, recommendations=[Row(userId=21, rating=7.619421005249023), Row(userId=25, rating=5.521111488342285), Row(userId=36, rating=5.4626593589782715), Row(userId=33, rating=5.411818981170654), Row(userId=16, rating=4.793625354766846), Row(userId=7, rating=4.692002773284912), Row(userId=46, rating=4.632454872131348), Row(userId=41, rating=4.475106239318848), Row(userId=40, rating=4.354674339294434), Row(userId=20, rating=4.208553791046143)]),\n",
       " Row(movieId=471, recommendations=[Row(userId=28, rating=4.085762977600098), Row(userId=24, rating=2.9981555938720703), Row(userId=7, rating=2.2341885566711426), Row(userId=8, rating=2.050200939178467), Row(userId=3, rating=1.9684978723526), Row(userId=34, rating=1.7562252283096313), Row(userId=6, rating=1.5029963254928589), Row(userId=35, rating=1.451230764389038), Row(userId=47, rating=1.4340031147003174), Row(userId=44, rating=1.3760236501693726)]),\n",
       " Row(movieId=1591, recommendations=[Row(userId=8, rating=4.002291679382324), Row(userId=36, rating=3.8705899715423584), Row(userId=11, rating=3.4943156242370605), Row(userId=44, rating=3.307216167449951), Row(userId=10, rating=3.2769365310668945), Row(userId=37, rating=3.1194748878479004), Row(userId=34, rating=3.061026096343994), Row(userId=49, rating=2.978318691253662), Row(userId=24, rating=2.934891700744629), Row(userId=33, rating=2.9313836097717285)])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieRecs.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|    title|\n",
      "+---+---------+\n",
      "|862|Toy Story|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.select('id','title').filter(\"title like '%Toy Story'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(recommendations=[Row(userId=21, rating=7.619421005249023), Row(userId=25, rating=5.521111488342285), Row(userId=36, rating=5.4626593589782715), Row(userId=33, rating=5.411818981170654), Row(userId=16, rating=4.793625354766846), Row(userId=7, rating=4.692002773284912), Row(userId=46, rating=4.632454872131348), Row(userId=41, rating=4.475106239318848), Row(userId=40, rating=4.354674339294434), Row(userId=20, rating=4.208553791046143)])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mitä tää data kertoo? Elokuvan perusteella lähinnä mun makua olevat käyttäjät?\n",
    "# Onko tää joku Tinderin korvike?\n",
    "movieRecs.filter(\"movieId = 1580\").select('recommendations').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Palataan takaisin manuaaliseen recommendation enginen koodaukseen..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pearson correlation test\n",
    "# Result... nulls are counted as zeros in Spark => crap results\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
    "        (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
    "        (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
    "        (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
    "\n",
    "data = [(Vectors.dense([7,6,7,4,5,4]),),\n",
    "        (Vectors.sparse(6, [(0,6), (1,7), (3,4), (4,3), (5,4) ]),),\n",
    "        (Vectors.sparse(6, [(1,3),(2,3),(3,1),(4,1)]),),\n",
    "        (Vectors.dense([1,2,2,3,3,4]),),\n",
    "        (Vectors.sparse(6, [(0,1),(2,1),(3,2),(4,3),(5,3)]),),\n",
    "        (Vectors.sparse(6, [(0,5),(1,4),(3,3),(5,4)]),)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "r1 = Correlation.corr(df, \"features\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n",
    "\n",
    "r2 = Correlation.corr(df, \"features\", \"spearman\").head()\n",
    "print(\"Spearman correlation matrix:\\n\" + str(r2[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This installs pyarrow to the conda_python3 kernel environment\n",
    "# conda install --yes --name python3 --channel conda-forge pyarrow\n",
    "#from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "# Use pandas_udf to define a Pandas UDF\n",
    "#@pandas_udf('double', PandasUDFType.SCALAR)\n",
    "# Input/output are both a pandas.Series of doubles\n",
    "\n",
    "#def pandas_plus_one(v):\n",
    "#    return v + 1\n",
    "\n",
    "#@pandas_udf('double', PandasUDFType.SCALAR)\n",
    "# Input/output are both a pandas.Series of doubles\n",
    "\n",
    "#def pandas_power2(v):\n",
    "#    return v*v\n",
    "\n",
    "#df.withColumn('v2', pandas_plus_one(df.v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "person1=2\n",
    "person1_square_preferences_sum = sum(\\\n",
    "                                     users_ratings_df(person1).\\\n",
    "filter(users_ratings_df(person1).movieId.isin(both_rated)).select('rating')*\\\n",
    "                                      users_ratings_df(person1).\\\n",
    "filter(users_ratings_df(person1).movieId.isin(both_rated)).select('rating')\\\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pearson correlation calculation with Spark join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+\n",
      "|movieId|rating1|rating2|\n",
      "+-------+-------+-------+\n",
      "|     32|    5.0|    2.0|\n",
      "|    260|    5.0|    4.0|\n",
      "|    377|    3.0|    4.0|\n",
      "|    648|    3.0|    4.0|\n",
      "|    780|    5.0|    3.0|\n",
      "|   1210|    5.0|    4.0|\n",
      "|   1552|    2.0|    2.0|\n",
      "+-------+-------+-------+\n",
      "\n",
      "+-------+-------+-------+---------+---------+---------------+\n",
      "|movieId|rating1|rating2|rating1^2|rating2^2|ratings_product|\n",
      "+-------+-------+-------+---------+---------+---------------+\n",
      "|     32|    5.0|    2.0|     25.0|      4.0|           10.0|\n",
      "|    260|    5.0|    4.0|     25.0|     16.0|           20.0|\n",
      "|    377|    3.0|    4.0|      9.0|     16.0|           12.0|\n",
      "|    648|    3.0|    4.0|      9.0|     16.0|           12.0|\n",
      "|    780|    5.0|    3.0|     25.0|      9.0|           15.0|\n",
      "|   1210|    5.0|    4.0|     25.0|     16.0|           20.0|\n",
      "|   1552|    2.0|    2.0|      4.0|      4.0|            4.0|\n",
      "+-------+-------+-------+---------+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Method to find common MovieIds\n",
    "from pyspark.sql.functions import *\n",
    "from math import sqrt \n",
    "person1_ratings = users_ratings_df(34) #.alias('df1')\n",
    "person2_ratings = users_ratings_df(2) #.alias('df2')\n",
    "\n",
    "#common_ratings = person1_ratings.withColumnRenamed(\"rating\", \"rating1\").join(person2_ratings.withColumnRenamed(\"rating\", \"rating2\"), \\\n",
    "\n",
    "# Join the ratings first, so that columns can be referred in calculations\n",
    "common_ratings = person1_ratings.toDF('movieId','rating1').join(person2_ratings.toDF('movieId','rating2'), \"movieId\")\n",
    "\n",
    "common_ratings_calc = common_ratings.select(common_ratings.movieId,common_ratings.rating1,common_ratings.rating2,\\\n",
    "                                            (pow(common_ratings.rating1,2)).alias('rating1^2'),\\\n",
    "                                            (pow(common_ratings.rating2,2)).alias('rating2^2'),\\\n",
    "                                            (common_ratings.rating1*common_ratings.rating2).alias('ratings_product')\\\n",
    "                                           )                                                                \n",
    "common_ratings.show()\n",
    "common_ratings_calc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1357241785076592\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from math import sqrt,pow \n",
    "\n",
    "def pearson_correlation(person1,person2):\n",
    "\n",
    "##### To get both rated items\n",
    "#    both_rated = {}\n",
    "    #for item in dataset[person1]:\n",
    "    #    if item in dataset[person2]:\n",
    "    #        both_rated[item] = 1\n",
    " \n",
    "#    for movieId,rating in users_ratings_df(person1).collect():\n",
    "#        if movieId in users_ratings_df(person2).collect():\n",
    "#            both_rated[movieId] = 1\n",
    "\n",
    "#    person1_ratings = users_ratings_df(34).alias('df1')\n",
    "#    person2_ratings = users_ratings_df(2).alias('df2')\n",
    "#    both_rated=person1_ratings.join(person2_ratings, df1.movieId == df2.movieId)\\\n",
    "#    .select('df1.*').select('movieId').rdd.flatMap(lambda x: x).collect()\n",
    "    \n",
    "#    return both_rated\n",
    "###########################\n",
    "\n",
    "\n",
    "#    number_of_ratings = len(both_rated)\n",
    "    \n",
    "    # Checking for number of ratings in common\n",
    "#    if number_of_ratings == 0:\n",
    "#        return 0\n",
    " \n",
    "    # Add up all the preferences of each user\n",
    "#    person1_preferences_sum = sum([dataset[person1][item] for item in both_rated])\n",
    "#    person2_preferences_sum = sum([dataset[person2][item] for item in both_rated])\n",
    "\n",
    "#    person1_preferences_sum = users_ratings_df(person1).\\\n",
    "#    filter(users_ratings_df(person1).movieId.isin(both_rated))\\\n",
    "#    .agg({\"rating\": \"sum\",\"rating\": \"avg\"}).select('sum(rating)').collect()[0][0]\n",
    "        \n",
    "#    person2_preferences_sum = users_ratings(person2).\\\n",
    "#    filter(users_ratings_df(person2).movieId.isin(both_rated))\\\n",
    "#    .agg({\"rating\": \"sum\"}).select('sum(rating)').collect()[0][0]\n",
    "    \n",
    "    \n",
    "    # Sum up the squares of preferences of each user\n",
    "#    person1_square_preferences_sum = sum([pow(dataset[person1][item],2) for item in both_rated])\n",
    "#    person2_square_preferences_sum = sum([pow(dataset[person2][item],2) for item in both_rated])\n",
    "\n",
    "#    person1_square_preferences_sum = sum(users_ratings_df(person1).\\\n",
    "#    filter(users_ratings_df(person1).movieId.isin(both_rated)).select('rating'),2)\n",
    " \n",
    " \n",
    "    # Sum up the product value of both preferences for each item\n",
    "#    product_sum_of_both_users = sum([dataset[person1][item] * dataset[person2][item] for item in both_rated])\n",
    "\n",
    "    \n",
    "    # Spark way of calculating all the above:\n",
    "    # Method to find common MovieIds\n",
    "    \n",
    "    person1_ratings = users_ratings_df(person1) #.alias('df1')\n",
    "    person2_ratings = users_ratings_df(person2) #.alias('df2')\n",
    "\n",
    "    # Join the ratings first, so that columns can be referred in calculations\n",
    "    common_ratings = person1_ratings.toDF('movieId','rating1').join(person2_ratings.toDF('movieId','rating2'), \"movieId\")\n",
    "\n",
    "    common_ratings_calc = common_ratings.select(common_ratings.movieId,common_ratings.rating1,common_ratings.rating2,\\\n",
    "                                            (common_ratings.rating1*common_ratings.rating1).alias('rating1^2'),\\\n",
    "                                            (common_ratings.rating2*common_ratings.rating2).alias('rating2^2'),\\\n",
    "                                            (common_ratings.rating1*common_ratings.rating2).alias('ratings_product')\\\n",
    "                                           )\n",
    "    \n",
    "    common_ratings_agg = common_ratings_calc.agg({\\\n",
    "                                             \"movieId\":\"count\",\\\n",
    "                                             \"rating1\": \"sum\",\\\n",
    "                                             \"rating2\": \"sum\",\\\n",
    "                                             \"rating1^2\": \"sum\",\\\n",
    "                                             \"rating2^2\": \"sum\",\\\n",
    "                                             \"ratings_product\": \"sum\"}).collect()[0]\n",
    "    \n",
    "    # Unpacking the numebers from the named tuple:\n",
    "    (number_of_ratings,\\\n",
    "     person1_preferences_sum,\\\n",
    "     person2_preferences_sum,\\\n",
    "     person1_square_preferences_sum,\\\n",
    "     person2_square_preferences_sum,\\\n",
    "     product_sum_of_both_users) = \\\n",
    "    (common_ratings_agg[\"count(movieId)\"],\\\n",
    "     common_ratings_agg[\"sum(rating1)\"],\\\n",
    "     common_ratings_agg[\"sum(rating2)\"],\\\n",
    "     common_ratings_agg[\"sum(rating1^2)\"],\\\n",
    "     common_ratings_agg[\"sum(rating2^2)\"],\\\n",
    "     common_ratings_agg[\"sum(ratings_product)\"],\\\n",
    "    )\n",
    "    \n",
    "    # Checking for number of common ratings\n",
    "    if number_of_ratings == 0:\n",
    "        return 0\n",
    "\n",
    "    # Calculate the pearson score\n",
    "    numerator_value = product_sum_of_both_users - (person1_preferences_sum*person2_preferences_sum/number_of_ratings)\n",
    "    denominator_value = sqrt((person1_square_preferences_sum - pow(person1_preferences_sum,2)/number_of_ratings) * (person2_square_preferences_sum -pow(person2_preferences_sum,2)/number_of_ratings))\n",
    "    if denominator_value == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        r = numerator_value/denominator_value\n",
    "        return r\n",
    "    \n",
    "print(pearson_correlation(2,34))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-bfafd55d668d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#prepare the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"temperature\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exhaust_vacuum\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ambient_pressure\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"relative_humidity\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlr_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"energy_output\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectorAssembler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "#From https://blog.epigno.systems/2018/02/21/machine-learning-with-pyspark-feature-selection/\n",
    "#prepare the data\n",
    "features = [\"temperature\", \"exhaust_vacuum\", \"ambient_pressure\", \"relative_humidity\"]\n",
    "lr_data = data.select(col(\"energy_output\").alias(\"label\"), *features).dropna()\n",
    "\n",
    "vector = VectorAssembler(inputCols=columns, outputCol=\"features\")\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "\n",
    "#stages = [vector, scaler]\n",
    "stages = [scaler]\n",
    "\n",
    "pipe = Pipeline(stages=stages)\n",
    "\n",
    "# we'll be using this data frame\n",
    "data_for_correlation = pipe.fit(lr_data).transform(lr_data).select(\"scaled_features\")\n",
    "\n",
    "\n",
    "#The correlation step\n",
    "correlation = Correlation.corr(data_for_correlation, \"scaled_features\", \"pearson\").collect()[0][0].toArray()\n",
    "\n",
    " # rename _1, _2 ... columns to their original name\n",
    "df = pd.DataFrame(correlation)\n",
    "df[\"features\"] = pd.Series(columns)\n",
    "\n",
    " # let's see the results\n",
    "display(spark.createDataFrame(df, schema=columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pearsonCorr = Correlation.corr(df, \"features\").collect()[0][0]\n",
    "print(str(pearsonCorr).replace('nan', 'NaN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
